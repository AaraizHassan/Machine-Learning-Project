{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Light Gradient Boost Machine for Prediction of Sale Price of Bulldozers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please install lightgbm if not already installed: \n",
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are importing all the necessary libraries we will be using in this code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step CSV dataset is loaded in a Pandas Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train.csv\", low_memory=False, parse_dates=[\"saledate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step SaleDate column is split into Year and Month. These features can be hepful in making predictions about the sale price.  \n",
    "Year is helpful as prices of used vehicles tend to lower over years, but newer vehicles become more expensive due to inflation. We hoped the model can capture trends from this information.  \n",
    "Month is potentially useful, as prices can wary according to season, especially in parts where construction may be unfeasible over the winters etc. and machines are going to be unused commodities for some months.  \n",
    "We also experimented with making columns for the Day, Day of the week, and Day of the year, but these lead to poorer performance of the model, most likely as the model would find it difficult to extract meaningful information from these and may be slightly overfitting to them. So we chose not to include those.  \n",
    "The original 'saledate' column is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"saleYear\"] = df.saledate.dt.year\n",
    "df[\"saleMonth\"] = df.saledate.dt.month\n",
    "# df[\"saleDay\"] = df.saledate.dt.day\n",
    "# df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n",
    "# df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n",
    "df.drop(\"saledate\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There currently a lot of missing values in the dataset. For some of these columns, missing values are unspecified if we observe that these columns already have entries that say \"None or Unspecified\".  \n",
    "So, we find any columns that contain such \"None or Unspecified\", and just replace the null values with \"None or Unspecified\" to complete these columns and ensure the rest of the information can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if (df[column] == 'None or Unspecified').any():\n",
    "        df[column] = df[column].fillna('None or Unspecified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still are many columns that have many missing values. We felt it would not be great to impute values in columns that have majority missing values, as whatever we impute with (mean, median, etc) could be misleading when the statistic is being calculate with such little information.  \n",
    "Therefore, we dropped columns with more than 75% missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: ['UsageBand', 'fiModelSeries', 'fiModelDescriptor', 'Stick', 'Engine_Horsepower', 'Track_Type', 'Grouser_Type', 'Differential_Type', 'Steering_Controls']\n"
     ]
    }
   ],
   "source": [
    "percentage_missing = df.isna().sum() / len(df) * 100\n",
    "columns_to_drop = percentage_missing[percentage_missing > 75].index\n",
    "\n",
    "print(\"Columns to be dropped:\", list(columns_to_drop))\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyzed the dataset in-depth, manually, using the \"Data Dictionary.xlsx\" provided by Kaggle with the dataset. We shortlisted many features that could be either useless or redundant (as in, features that were one-to-one related to other features) and experimented with dropping some of them.  \n",
    "Somewhat counter-intuitively, dropping most of these columns made our model performance ever so slightly worse on the RMSLE. So, we chose to keep most of them and let the model learn by itself, and only dropped the few that showed an actual improvement.  \n",
    "Columns we experimented with manually dropping include:  \n",
    "'SalesID', 'MachineID', 'state', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiProductClassDesc', 'datasource', 'auctioneerID', 'ProductGroupDesc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop2 = ['datasource', 'auctioneerID', 'ProductGroupDesc']\n",
    "\n",
    "df = df.drop(columns=columns_to_drop2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step values that are still missing are being catered.\n",
    "1. Numeric columns with missing values are imputed with the median. It is because medican is less sensitive to outliers compared to mean. \n",
    "2. Categorical columns are imputed with the most frequent value in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "df[df.select_dtypes(include=['float64']).columns] = numeric_imputer.fit_transform(df.select_dtypes(include=['float64']))\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[df.select_dtypes(include=['object']).columns] = categorical_imputer.fit_transform(df.select_dtypes(include=['object']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconding of Categorical Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns in the dataset are selected and and coverted in to numeric form using label encoding. It is because like many other algorithms LGBM also required data to be in the numeric form.\n",
    "Label encoding transforms categorical data into a simple numerical format that is efficient in terms of memory and computation as our dataset is very large around 401k rows, it seemed to be a suitable choice over one-hot encoding (which increase the size of dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = LabelEncoder().fit_transform(df[column])\n",
    "\n",
    "# df = pd.get_dummies(df, columns=df.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into X -> Features and y -> labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the data is split into training and validation sets using train_test_split and there sizes are printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320900, 41)\n",
      "(320900,)\n",
      "(80225, 41)\n",
      "(80225,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingg LGBM Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lgb.Dataset function converts this data into a format that is internally optimized for speed and memory usage by LightGBM.\n",
    "LightGBM's advanced features like handling categorical features, optimizing memory usage, and speeding up training are largely due to this specialized data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step two things are done:\n",
    "1. The parameters of the LGBM are deifined. These include the type of model (gbdt for gradient boosted decision trees), the objective (regression), Metric:{'l2', 'l1'} means it uses both the L2 (mean squared error) and L1 (mean absolute error) metrics for regression. L2 is sensitive to outliers, whereas L1 is more robust to them, num_leaves specifies the maximum number of leaves in one tree. It is the key parameter that controls the complexity of the model, learning rate etc.\n",
    "2. LGBM model is trainined on X_train. num_boost_round indicates the number of boosting iterations. Early_stopping_rounds is used to stop training if the validation score does not improve for 5 consecutive rounds, which helps in preventing overfitting.\n",
    "\n",
    "Key Note: Increasing the num_boost_round decreased the rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's l1: 6739.08\tvalid_0's l2: 9.97596e+07\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=90,\n",
    "                valid_sets=lgb_eval,\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=5)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction and Calculating RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is now used to predict the sale price on the validation set. We are using RMSLE to evaluate model performance. The RMSLE is a measure of the ratio between the actual and predicted values. A smaller RMSLE value means better performance, with 0 being the ideal score indicating perfect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on validation set: 0.31593054635125856\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmsle = np.sqrt(mean_squared_error(np.log1p(y_val), np.log1p(y_pred)))\n",
    "print(\"RMSLE on validation set:\", rmsle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
